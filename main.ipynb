{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms,utils\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"lib/\")\n",
    "import model\n",
    "import data_handle\n",
    "import random\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper and Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 27  # Number of amino acids (replace with your actual input dimension)\n",
    "output_dim = 2  # Binary labels\n",
    "hidden_dim = 128\n",
    "num_layers = 1\n",
    "num_heads = 8\n",
    "EPOCH = 300\n",
    "PRETRAIN = False\n",
    "ALPHA = 20\n",
    "alpha = 1/ALPHA\n",
    "MODELNAME = f\"Alpha{ALPHA}_8Head_128Hidden_Start_end\"\n",
    "# latest_model -> Path to pretrained weight\n",
    "latest_model = None\n",
    "if PRETRAIN :\n",
    "    START,_ = latest_model.split('.')\n",
    "    _,START = START.split(\"_\")\n",
    "    START = int(START)\n",
    "else:\n",
    "    START= 0\n",
    "TRAIN = \"data/ATP_train.txt\"\n",
    "TEST = \"data/ATP_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(f\"mkdir Model\\{MODELNAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data_handle.ProteinDataset(TRAIN)\n",
    "train_set,val_set = torch.utils.data.random_split(train_set,[280,67])\n",
    "train_loader = data.DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "val_loader = data.DataLoader(val_set, batch_size=1, shuffle=False)\n",
    "test_set = data_handle.ProteinDataset(TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Initialize CUDA device\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = {'state_dict': model.state_dict(),\n",
    "             'optimizer' : optimizer.state_dict()}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    #print('model saved to %s' % checkpoint_path)\n",
    "    \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    #print('model loaded from %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_loader,val_loader,optimizer,epochs): \n",
    "    val_log = []\n",
    "    train_log=[]\n",
    "    acc_log = [] \n",
    "    rc_log = []\n",
    "    weights = torch.tensor([alpha,1-alpha]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    c_time = time.time()\n",
    "    N_train = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0 \n",
    "        for sequences, labels in train_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            y_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "            loss = criterion(outputs.view(-1, output_dim), y_tensor.view(-1))\n",
    "            #loss = criterion(outputs.view(-1, output_dim), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #record  \n",
    "        acc,rc,val_loss = test(model,val_loader,criterion)\n",
    "        acc_log.append(acc)\n",
    "        rc_log.append(rc)\n",
    "        val_log.append(val_loss)\n",
    "        train_log.append(total_loss/N_train)\n",
    "        print(f\"[{epoch}/{epochs}] Epoch {epoch+1}, Average Train Loss: {total_loss/N_train} , Average Validation Loss:{val_loss}\",end='\\r')\n",
    "    \n",
    "\n",
    "        if (epoch+1)%5 == 0:\n",
    "            path = f\"Model/{MODELNAME}/{MODELNAME}_{epoch+1+START}.pth\"\n",
    "            save_checkpoint(path,model,optimizer)\n",
    "        \n",
    "        \n",
    "    print(\"\\ntime spent: \",time.time()-c_time)\n",
    "\n",
    "    return acc_log,rc_log,train_log,val_log\n",
    "\n",
    "def test(model,data_loader,criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    N = len(data_loader.dataset)\n",
    "    #turn off gradient calculation, for computation speed\n",
    "    tp=fp=tn=fn = 0 \n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for data,target in data_loader:\n",
    "            data,target = data.to(device),target.to(device)\n",
    "            outputs = model(data)\n",
    "            y_tensor = torch.tensor(target, dtype=torch.long)\n",
    "            loss = criterion(outputs.view(-1, output_dim), y_tensor.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            #Evaluate on accuracy \n",
    "            pred = outputs.argmax(dim=2)\n",
    "            c = pred/y_tensor\n",
    "            tp += torch.sum(c == 1).item()\n",
    "            fp += torch.sum(c == float('inf')).item()\n",
    "            tn += torch.sum(torch.isnan(c)).item()\n",
    "            fn += torch.sum(c == 0).item()\n",
    "        average_loss = total_loss/len(data_loader.dataset)\n",
    "        acc = (tp+tn)/(tp+tn+fn+fp)\n",
    "        recall = (tp)/(tp+fn)\n",
    "    #pred = outputs.argmax(dim=2)\n",
    "    #print(f\"Average Loss Validation {N} Image: {average_loss}\")\n",
    "    #print(f\"Accuracy = {tp/N}\")\n",
    "    return (acc,recall,average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Latest Model and continue Training (Or Train from Scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\徐嘉祥\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = model.TransformerClassifier(input_dim, hidden_dim, output_dim, num_layers, num_heads)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "if PRETRAIN:\n",
    "    load_checkpoint(latest_model,model,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\徐嘉祥\\Desktop\\Course\\Bio-infor\\BIO\\Weighted_loss\\lib\\data_handle.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.IntTensor(sequence), torch.tensor(label)\n",
      "C:\\Users\\徐嘉祥\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "C:\\Users\\徐嘉祥\\AppData\\Local\\Temp\\ipykernel_7996\\660526941.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(labels, dtype=torch.long)\n",
      "C:\\Users\\徐嘉祥\\AppData\\Local\\Temp\\ipykernel_7996\\660526941.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tensor = torch.tensor(target, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299/300] Epoch 300, Average Train Loss: 0.09695379939283677 , Average Validation Loss:3.9963394087641997\n",
      "time spent:  189.74892330169678\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "acc_log,rc_log,train_log,val_log = train_model(model,train_loader,val_loader,optimizer,EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTRAIN_LOSS = f\"loss_log/train_loss_{EPOCH+START}\"\\nVAL_LOSS = f\"loss_log/val_loss_{EPOCH+START}\"\\nwith open(TRAIN_LOSS, \\'wb\\') as f:\\n    pk.dump(train_log, f)\\nwith open(VAL_LOSS, \\'wb\\') as f:\\n    pk.dump(val_log, f)\\n    '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pk\n",
    "'''\n",
    "TRAIN_LOSS = f\"loss_log/train_loss_{EPOCH+START}\"\n",
    "VAL_LOSS = f\"loss_log/val_loss_{EPOCH+START}\"\n",
    "with open(TRAIN_LOSS, 'wb') as f:\n",
    "    pk.dump(train_log, f)\n",
    "with open(VAL_LOSS, 'wb') as f:\n",
    "    pk.dump(val_log, f)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha20_8Head_128Hidden_Start_end\n"
     ]
    }
   ],
   "source": [
    "meta = {}\n",
    "meta['input_dim'] = input_dim\n",
    "meta['output_dim'] = output_dim\n",
    "meta['hidden_dim'] = hidden_dim\n",
    "meta['num_layers'] = num_layers\n",
    "meta['num_heads'] = num_heads\n",
    "train_result = {}\n",
    "train_result['acc_log'] = acc_log\n",
    "train_result['rc_log'] = rc_log\n",
    "train_result['train_log'] = train_log\n",
    "train_result['val_log'] = val_log\n",
    "train_result['meta'] = meta\n",
    "META = f'loss_log/{MODELNAME}'\n",
    "with open(META, 'wb') as f:\n",
    "    pk.dump(train_result, f)\n",
    "print(MODELNAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
